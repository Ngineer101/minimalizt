---
title: "What is an MCP server and why you should build one"
description: "The Model Context Protocol (MCP) turns your API into a first-class AI tool. Learn what MCP is, how it works, and why building an MCP server today can help grow your product."
pubDate: "Jun 11 2025"
heroImage: "/blog-cover-1.jpeg"
tags: ["AI", "Opinion", "Tech"]
---

If you have used modern AI assistants recently — ChatGPT, Claude, Windsurf, Cursor — you have probably noticed that they can take actions on your behalf: run a SQL query, create a GitHub issue, or even deploy infrastructure. Behind the scenes, those assistants are not guessing at random HTTP calls, they are invoking well-defined “tools” exposed via the **Model Context Protocol (MCP)** of the integrated platforms.

In short, MCP is a thin specification that makes an API self-describing to large language model agents. Once an endpoint is wrapped as an MCP tool, _any_ compliant AI client can discover it, decide when to call it, and pass correctly typed arguments — all from plain English prompts. Instead of integrating with a system by writing code (which requires a wide range of knowledge), you can just connect an MCP server, add your access token, and integrate using natural language.

## What exactly _is_ MCP?

The [**Model Context Protocol**](https://modelcontextprotocol.io/) is an emerging open standard (originally proposed by Anthropic) that sits on top of your existing API. Each endpoint - name, parameters, and response schema - is described in a machine-readable manifest, typically autogenerated by the SDK from your code. The manifest is served by an **MCP server** over either STDIO (for local tools) or HTTP (for remote services).

Because the contract is explicit, an AI model can load the manifest into its context window and reason about how to call your function, just like developers reason about typed function signatures in code.

Instead of making an API request, you can just type "list all the projects in my account", and all your projects will be listed.

## Why bother building one?

Building an MCP server adds surprisingly little surface area to your stack but unlocks three major benefits:

- **A new distribution channel.** Every AI app that speaks MCP instantly becomes an integration partner. Your API can be discovered inside familiar AI interfaces, like ChatGPT’s tool palette or Claude’s “extensions” tab. These integrations unlock a high-potential distribution channel.
- **Zero-friction onboarding.** Non-technical users can _talk_ to your API instead of reading docs, finding SDK keys, and writing boilerplate. The AI handles the call semantics for them, making your application instantly accessible to a much wider audience.
- **Higher-value feedback.** Because requests come through structured tools, you get rich telemetry: what users asked, which arguments were passed, and where they struggled. This is powerful input for both product and model improvement.

## How does authentication work?

Authentication is transport-specific, but MCP intentionally mirrors existing patterns. Two transports are supported:

| Transport | Typical Use-case                  | Auth Strategy                                                          |
| --------- | --------------------------------- | ---------------------------------------------------------------------- |
| **STDIO** | Local CLI helpers, dev-only tools | Share an environment variable or read from a local config file.        |
| **HTTP**  | SaaS APIs, multi-tenant services  | Issue user-scoped bearer tokens via OAuth (the spec re-uses RFC 6749). |

Most assistants today ship with token-based prompting UIs, so once your server advertises its auth URL, they can guide the user through the flow automatically.

## Real-world examples

1. **Supabase MCP** – Lets ChatGPT manage Postgres tables and Edge Functions without opening the dashboard.
2. **Vercel MCP** – Deploy, view logs, or roll back a Next.js app entirely via chat.
3. **Stripe MCP (community)** – Generate payment links or refund a charge by describing the intent in natural language.

These are early days, but the pattern is already proving itself across infrastructure, DevOps, and productivity tooling.

## Getting started

1. **Write your server code.** Use the MCP SDK (such as Python’s `mcp` package) to define your tools with type hints and docstrings. The SDK will autogenerate the manifest describing your endpoints.
2. **Run your MCP server.** For HTTP, serve the manifest at `/mcp.json` and handle tool invocations via HTTP. For STDIO, the server prints the manifest on startup and processes JSON-style invocations from stdin.
3. **Connect your server to a client.** Most tools (like Claude, Windsurf, or others) let you add the server by URL or local process—no public listing required, though you can submit to marketplaces for broader discovery.

Total time: often _under an hour_ for a small API.

## Final thoughts

MCP is not the final word on AI integrations, but it is today’s best lightweight standard—and it is moving fast. Early adopters are already capturing organic traffic from AI interfaces, gathering fine-grained usage data, and shortening their onboarding funnels.

If your product exposes an API, shipping an MCP server is one of the highest-leverage tasks you can tackle this quarter. Your future users will _talk_ to software; make sure your software can _listen_.
